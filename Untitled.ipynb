{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d811ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7476441",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './matrix/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "764289e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[success] M1 generated and saved.\n"
     ]
    }
   ],
   "source": [
    "n= 5000\n",
    "m = 1000\n",
    "M1 = np.random.randn(n, m)\n",
    "np.savetxt(PATH + \"m1.txt\", M1)\n",
    "print(\"[success] M1 generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f01a6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.loadtxt(PATH + 'm1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b61bdc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f375416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class normFunction():\n",
    "    def __init__(self, A):\n",
    "        self.A = A\n",
    "        # Q = A'A \n",
    "        self.Q = np.matmul(np.transpose(A), A)\n",
    "        print(self.Q.shape)\n",
    "        self.dim = self.Q.shape[0]\n",
    "\n",
    "    # fucntion to calculate the function f(x) and it's gradient grad(x)\n",
    "    def calculate(self, x):\n",
    "        # f(x) = x'Qx / x'x\n",
    "        self.x = x\n",
    "        self.xT = x.T\n",
    "        self.xTx = np.matmul(self.xT, x)\n",
    "        self.Qx = np.matmul(self.Q, x)\n",
    "        self.xQx = np.matmul(self.xT, self.Qx)\n",
    "        f_x = self.xQx / self.xTx\n",
    "        nabla_f = (2 * x * f_x) / self.xTx - (2 * self.Qx) / self.xTx\n",
    "        # nabla_f = (2 * self.Qx) / self.xTx - (2 * x * f_x) / self.xTx\n",
    "        self.f_x = f_x  # it's -f(x)\n",
    "        self.d = nabla_f\n",
    "        self.dT = self.d.T\n",
    "        return f_x, nabla_f\n",
    "\n",
    "    # funciton that return the step size of the algorithm using exact line search along the \n",
    "    # graient direction.\n",
    "    def stepsizeAlongGradientDirection(self):\n",
    "        dTd = np.matmul(self.dT, self.d)\n",
    "        xTd = np.matmul(self.xT, self.d)\n",
    "        dTx = np.matmul(self.dT, self.x)\n",
    "        self.xTx = np.matmul(self.xT, self.x)\n",
    "        Qd = np.matmul(self.Q, self.d)\n",
    "        dQ = Qd.T\n",
    "        xQd = np.matmul(self.xT, Qd)\n",
    "        dQx = np.matmul(dQ, self.x)\n",
    "        dQd = np.matmul(self.dT, Qd)\n",
    "        # a = (d.T*d)(x*Q*d) - (d.T*Q*d)*(x.T*d)  \n",
    "        a = float(dTd * xQd - dQd * xTd)\n",
    "        # a = float(dQd * dTx - dTd * dQx) #ours\n",
    "        # b = (xTx)(dQd) - (dTd)(xQx)\n",
    "        b = float(self.xTx * dQd - dTd * self.xQx)\n",
    "        # b = float(self.xTx * dQd - dTd * self.xQx) #ours\n",
    "        # c = (xTd)(xQx) - (xTx)(xQd)\n",
    "        c = float(xTd * self.xQx - self.xTx * xQd)\n",
    "        # c = float(self.xTx * dQx - xTd * self.xQx) #ours\n",
    "        # now alpha is the solution of ax^2+bx+c : x > 0 \n",
    "        coef = np.array([a, b, c])\n",
    "        roots = np.roots(coef)\n",
    "        if roots[0] < 0 and roots[1] < 0:\n",
    "            return 0\n",
    "        elif roots[0] < 0:\n",
    "            return roots[1]\n",
    "        elif roots[1] < 0:\n",
    "            return roots[0]\n",
    "        return np.min([roots])\n",
    "    \n",
    "    def init_x(self):\n",
    "        return np.random.rand(self.dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "549f9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as la\n",
    "\n",
    "class steepestGradientDescent():\n",
    "    def __init__(self, function, eps, maxIter, x=None, verbose = True):\n",
    "        self.verbose = verbose\n",
    "        self.function = function\n",
    "        self.status = ''\n",
    "        self.feval = 1\n",
    "        self.eps = eps\n",
    "        self.maxIter = maxIter\n",
    "        self.x = x if x is not None else self.function.init_x()\n",
    "\n",
    "        self.v, self.g = function.calculate(self.x)\n",
    "        self.ng = la.norm(self.g)\n",
    "        # Absolute error or relative error?\n",
    "        if self.eps < 0:\n",
    "            self.ng0 = - self.ng\n",
    "        else:\n",
    "            self.ng0 = 1\n",
    "\n",
    "    def steepestGradientDescent(self):\n",
    "        self.historyNorm = []\n",
    "        self.historyValue = []\n",
    "        while True:\n",
    "            self.historyNorm.append(float(self.ng))\n",
    "            self.historyValue.append(float(self.v))\n",
    "            if self.verbose: \n",
    "                self.print()\n",
    "\n",
    "            # Norm of the gradient lower or equal of the epsilon\n",
    "            if self.ng <= self.eps * self.ng0:\n",
    "                self.status = 'optimal'\n",
    "                if self.verbose:\n",
    "                    self.print()\n",
    "                return self.historyNorm, self.historyValue\n",
    "\n",
    "\n",
    "            # Man number of iteration?\n",
    "            if self.feval > self.maxIter:\n",
    "                self.status = 'stopped'\n",
    "                if self.verbose:\n",
    "                    self.print()\n",
    "                return self.historyNorm, self.historyValue\n",
    "\n",
    "            # calculate step along direction\n",
    "            alpha = self.function.stepsizeAlongGradientDirection()\n",
    "\n",
    "            # step too short\n",
    "            # if alpha <= conf.mina:\n",
    "            if alpha <= 1e-16:\n",
    "                self.status = 'error'\n",
    "                if self.verbose:\n",
    "                    self.print()\n",
    "                return self.historyNorm, self.historyValue\n",
    "\n",
    "            lastx = self.x\n",
    "            self.x = self.x - alpha * self.g\n",
    "            self.v, self.g = self.function.calculate(self.x)\n",
    "            self.feval = self.feval + 1\n",
    "\n",
    "            # if self.v <= conf.MInf:\n",
    "            if self.v <= - float(\"inf\"):\n",
    "                self.status = 'unbounded'\n",
    "                if self.verbose:\n",
    "                    self.print()\n",
    "                return self.historyNorm, self.historyValue\n",
    "\n",
    "            self.ng = la.norm(self.g)\n",
    "\n",
    "        print('\\n x = ' + str(self.x) + '\\nvalue = %0.4f' % self.v)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Iterations number %d, -f(x) = %0.4f, gradientNorm = %f - \" % (self.feval, self.v, self.ng) + self.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "307eaab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "relerrorsSGD = []\n",
    "gradientsSGD = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5b052de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "f = normFunction(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1410ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_vector = f.init_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e645d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations number 1, -f(x) = 4904.3692, gradientNorm = 244.745522 - \n",
      "Iterations number 2, -f(x) = 7471.5107, gradientNorm = 148.284294 - \n",
      "Iterations number 3, -f(x) = 8591.0875, gradientNorm = 91.157519 - \n",
      "Iterations number 4, -f(x) = 6989.4723, gradientNorm = 63.519614 - \n",
      "Iterations number 5, -f(x) = 9589.3859, gradientNorm = 16.566243 - \n",
      "Iterations number 6, -f(x) = 4372.3532, gradientNorm = 8.440849 - \n",
      "Iterations number 7, -f(x) = 8068.4893, gradientNorm = 7.276552 - \n",
      "Iterations number 8, -f(x) = 8551.8230, gradientNorm = 3.653697 - \n",
      "Iterations number 9, -f(x) = 7444.8561, gradientNorm = 2.004565 - \n",
      "Iterations number 10, -f(x) = 9341.4585, gradientNorm = 0.772206 - \n",
      "Iterations number 11, -f(x) = 4974.3047, gradientNorm = 0.398168 - \n",
      "Iterations number 12, -f(x) = 9604.7485, gradientNorm = 0.166687 - \n",
      "Iterations number 13, -f(x) = 4364.4146, gradientNorm = 0.078901 - \n",
      "Iterations number 14, -f(x) = 8662.2951, gradientNorm = 0.057739 - \n",
      "Iterations number 15, -f(x) = 7246.4145, gradientNorm = 0.031517 - \n",
      "Iterations number 16, -f(x) = 9629.9761, gradientNorm = 0.010610 - \n",
      "Iterations number 17, -f(x) = 4148.3095, gradientNorm = 0.004955 - \n",
      "Iterations number 18, -f(x) = 8555.2140, gradientNorm = 0.003771 - \n",
      "Iterations number 19, -f(x) = 7315.9714, gradientNorm = 0.002030 - \n",
      "Iterations number 20, -f(x) = 9490.6938, gradientNorm = 0.000733 - \n",
      "Iterations number 21, -f(x) = 4582.1135, gradientNorm = 0.000364 - \n",
      "Iterations number 22, -f(x) = 9331.1413, gradientNorm = 0.000197 - \n",
      "Iterations number 23, -f(x) = 5141.3034, gradientNorm = 0.000103 - \n",
      "Iterations number 24, -f(x) = 9922.2971, gradientNorm = 0.000033 - \n",
      "Iterations number 25, -f(x) = 3237.7761, gradientNorm = 0.000013 - \n",
      "Iterations number 26, -f(x) = 6576.0883, gradientNorm = 0.000015 - \n",
      "Iterations number 27, -f(x) = 10165.5386, gradientNorm = 0.000003 - \n",
      "Iterations number 28, -f(x) = 2261.7791, gradientNorm = 0.000001 - \n",
      "Iterations number 28, -f(x) = 2261.7791, gradientNorm = 0.000001 - optimal\n"
     ]
    }
   ],
   "source": [
    "# Optimizer SGD\n",
    "optimizerSGD = steepestGradientDescent(f, 1e-6, 500, x=initial_vector, verbose = True)\n",
    "gradientSGD, normsSGD = optimizerSGD.steepestGradientDescent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57fc91bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10375.86945983966\n"
     ]
    }
   ],
   "source": [
    "# Norm numpy\n",
    "norm = la.norm(A, ord=2) ** 2\n",
    "print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4b42710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm and errors SGD\n",
    "normsSGD = np.array(normsSGD)\n",
    "gradientsSGD.insert(0,np.array(gradientSGD))\n",
    "size1 = normsSGD.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ded25634",
   "metadata": {},
   "outputs": [],
   "source": [
    "normvec = np.ones(size1) * norm\n",
    "relerrorsSGD.insert(0, (abs(normsSGD - normvec) / abs(normvec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86582008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36399146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm",
   "language": "python",
   "name": "cm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
